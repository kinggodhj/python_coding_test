{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='./mnist_train.csv'\n",
    "test_path='./mnist_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data=[]\n",
    "    label=[]\n",
    "    f = open(path, 'r', encoding='utf-8')\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        label.append(int(line[0]))\n",
    "        data.append(list(map(int, line[1:])))\n",
    "    f.close()   \n",
    "    return label, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label, train_data=load_data(train_path)\n",
    "train_data=np.reshape(train_data, (-1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label, test_data=load_data(test_path)\n",
    "test_data=np.reshape(test_data, (-1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding\n",
    "train_label=np.eye(class_num)[train_label]\n",
    "test_label=np.eye(class_num)[test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "check=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(train_data[check]) \n",
    "print('label:', train_label[check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "\n",
    "train_data=train_data/255\n",
    "test_data=test_data/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP (One layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__(self, batch, lr):\n",
    "        self.batch=batch\n",
    "        self.w1=np.random.rand(28*28, 10)\n",
    "        self.b1=np.random.rand(10)\n",
    "        self.lr=lr\n",
    "        \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def sigmoid(self, x, der=False):\n",
    "        if der==False:\n",
    "            return 1/(1+np.exp(-1*x))\n",
    "        else:\n",
    "            return (1/(1+np.exp(-1*x)))*(1- (1/(1+np.exp(-1*x))))\n",
    "    \n",
    "    def softmax(self, x, der=False):\n",
    "        ex=np.exp(x)\n",
    "        return ex/np.sum(ex) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x=x.reshape(-1)\n",
    "        self.net=np.dot(self.x, self.w1) + self.b1\n",
    "        prob=self.softmax(self.net)\n",
    "        return prob\n",
    "    \n",
    "    def backward(self, pred, label, g1, b1):\n",
    "        err1=pred-label\n",
    "        g1+=np.matmul(np.transpose(self.x).reshape(-1,1), err1.reshape(1, -1))\n",
    "        b1+=err1\n",
    "        return g1, b1\n",
    "        \n",
    "    def update(self, g1,b1):\n",
    "        self.w1-=self.lr*g1/self.batch\n",
    "        self.b1-=self.lr*b1.reshape(-1)/self.batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(y, label):\n",
    "    delta=int(1e-7)\n",
    "    return -1*np.sum(label*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=128\n",
    "lr=0.001\n",
    "layer=MLP(batch, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=train_data[len(train_data)//batch*batch:]\n",
    "val_label=train_label[len(train_data)//batch*batch:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "epoch: 0\n",
      "iter: 0 loss: 4.399618861583656\n",
      "iter: 100 loss: 4.133578031741354\n",
      "iter: 200 loss: 3.361099880695041\n",
      "iter: 300 loss: 3.5012170163381118\n",
      "iter: 400 loss: 2.467284236687878\n",
      "Acc: 0.14583333333333334\n",
      "###########################################\n",
      "epoch: 1\n",
      "iter: 0 loss: 2.816144971388114\n",
      "iter: 100 loss: 3.163026274043697\n",
      "iter: 200 loss: 2.6645751314542965\n",
      "iter: 300 loss: 2.8965647157465364\n",
      "iter: 400 loss: 2.108565745360322\n",
      "Acc: 0.1875\n",
      "###########################################\n",
      "epoch: 2\n",
      "iter: 0 loss: 2.3399966351906905\n",
      "iter: 100 loss: 2.714589057886846\n",
      "iter: 200 loss: 2.2838988169214773\n",
      "iter: 300 loss: 2.5081453185261022\n",
      "iter: 400 loss: 1.8555418583375063\n",
      "Acc: 0.2604166666666667\n",
      "###########################################\n",
      "epoch: 3\n",
      "iter: 0 loss: 2.0134802677415506\n",
      "iter: 100 loss: 2.3805127267695467\n",
      "iter: 200 loss: 1.9993573851166384\n",
      "iter: 300 loss: 2.2071843919789953\n",
      "iter: 400 loss: 1.659838994381156\n",
      "Acc: 0.34375\n",
      "###########################################\n",
      "epoch: 4\n",
      "iter: 0 loss: 1.7677272876161074\n",
      "iter: 100 loss: 2.117622222239165\n",
      "iter: 200 loss: 1.7775418471268678\n",
      "iter: 300 loss: 1.9710534903174415\n",
      "iter: 400 loss: 1.5097450568365243\n",
      "Acc: 0.3854166666666667\n",
      "###########################################\n",
      "epoch: 5\n",
      "iter: 0 loss: 1.5800065405479866\n",
      "iter: 100 loss: 1.907652198218267\n",
      "iter: 200 loss: 1.6019903490586302\n",
      "iter: 300 loss: 1.784193035961\n",
      "iter: 400 loss: 1.3925847672768803\n",
      "Acc: 0.4479166666666667\n",
      "###########################################\n",
      "epoch: 6\n",
      "iter: 0 loss: 1.4341095955269558\n",
      "iter: 100 loss: 1.7376623876130104\n",
      "iter: 200 loss: 1.460948766358953\n",
      "iter: 300 loss: 1.6341899013008738\n",
      "iter: 400 loss: 1.2987084892716818\n",
      "Acc: 0.4791666666666667\n",
      "###########################################\n",
      "epoch: 7\n",
      "iter: 0 loss: 1.3181810072314042\n",
      "iter: 100 loss: 1.598069180479836\n",
      "iter: 200 loss: 1.3457727210606372\n",
      "iter: 300 loss: 1.511743528959795\n",
      "iter: 400 loss: 1.22167693397895\n",
      "Acc: 0.4895833333333333\n",
      "###########################################\n",
      "epoch: 8\n",
      "iter: 0 loss: 1.223976957052821\n",
      "iter: 100 loss: 1.4818388738052364\n",
      "iter: 200 loss: 1.2501830995759327\n",
      "iter: 300 loss: 1.4101219471681818\n",
      "iter: 400 loss: 1.1572216991489754\n",
      "Acc: 0.5104166666666666\n",
      "###########################################\n",
      "epoch: 9\n",
      "iter: 0 loss: 1.145872015534862\n",
      "iter: 100 loss: 1.3838105042939328\n",
      "iter: 200 loss: 1.1696499541568124\n",
      "iter: 300 loss: 1.3244919219443143\n",
      "iter: 400 loss: 1.102428355387067\n",
      "Acc: 0.53125\n",
      "###########################################\n",
      "epoch: 10\n",
      "iter: 0 loss: 1.079997776855871\n",
      "iter: 100 loss: 1.3001687124577428\n",
      "iter: 200 loss: 1.1008925803862548\n",
      "iter: 300 loss: 1.2513591324775524\n",
      "iter: 400 loss: 1.055239531879604\n",
      "Acc: 0.53125\n",
      "###########################################\n",
      "epoch: 11\n",
      "iter: 0 loss: 1.023637580096832\n",
      "iter: 100 loss: 1.2280600733794684\n",
      "iter: 200 loss: 1.041506516532258\n",
      "iter: 300 loss: 1.188161348676651\n",
      "iter: 400 loss: 1.0141596093133687\n",
      "Acc: 0.5416666666666666\n",
      "###########################################\n",
      "epoch: 12\n",
      "iter: 0 loss: 0.9748349934647682\n",
      "iter: 100 loss: 1.165321067251\n",
      "iter: 200 loss: 0.9897004284029051\n",
      "iter: 300 loss: 1.1329882110222227\n",
      "iter: 400 loss: 0.9780731989998167\n",
      "Acc: 0.5520833333333334\n",
      "###########################################\n",
      "epoch: 13\n",
      "iter: 0 loss: 0.9321457932372473\n",
      "iter: 100 loss: 1.1102871159074916\n",
      "iter: 200 loss: 0.9441155186149855\n",
      "iter: 300 loss: 1.0843908248197967\n",
      "iter: 400 loss: 0.9461291033913909\n",
      "Acc: 0.5729166666666666\n",
      "###########################################\n",
      "epoch: 14\n",
      "iter: 0 loss: 0.8944795001438668\n",
      "iter: 100 loss: 1.0616588284033626\n",
      "iter: 200 loss: 0.9037025446794645\n",
      "iter: 300 loss: 1.0412518640805508\n",
      "iter: 400 loss: 0.9176638801189454\n",
      "Acc: 0.6145833333333334\n",
      "###########################################\n",
      "epoch: 15\n",
      "iter: 0 loss: 0.8609960182287383\n",
      "iter: 100 loss: 1.0184079442138612\n",
      "iter: 200 loss: 0.8676376917142805\n",
      "iter: 300 loss: 1.0026960044029412\n",
      "iter: 400 loss: 0.8921504200026507\n",
      "Acc: 0.6354166666666666\n",
      "###########################################\n",
      "epoch: 16\n",
      "iter: 0 loss: 0.831036552443368\n",
      "iter: 100 loss: 0.9797105689762945\n",
      "iter: 200 loss: 0.8352643311790049\n",
      "iter: 300 loss: 0.9680273672451711\n",
      "iter: 400 loss: 0.8691627791101869\n",
      "Acc: 0.6354166666666666\n",
      "###########################################\n",
      "epoch: 17\n",
      "iter: 0 loss: 0.8040763044038718\n",
      "iter: 100 loss: 0.944899148630307\n",
      "iter: 200 loss: 0.8060520466809461\n",
      "iter: 300 loss: 0.9366852367259637\n",
      "iter: 400 loss: 0.8483517495571642\n",
      "Acc: 0.6458333333333334\n",
      "###########################################\n",
      "epoch: 18\n",
      "iter: 0 loss: 0.7796913429407378\n",
      "iter: 100 loss: 0.9134273884835643\n",
      "iter: 200 loss: 0.7795672884175833\n",
      "iter: 300 loss: 0.9082122739901961\n",
      "iter: 400 loss: 0.829427595365857\n",
      "Acc: 0.6770833333333334\n",
      "###########################################\n",
      "epoch: 19\n",
      "iter: 0 loss: 0.7575349257344438\n",
      "iter: 100 loss: 0.8848442236462842\n",
      "iter: 200 loss: 0.7554519732949854\n",
      "iter: 300 loss: 0.882231373957655\n",
      "iter: 400 loss: 0.8121476033434466\n",
      "Acc: 0.6875\n",
      "###########################################\n",
      "epoch: 20\n",
      "iter: 0 loss: 0.7373202660757843\n",
      "iter: 100 loss: 0.8587742174042643\n",
      "iter: 200 loss: 0.733407607337837\n",
      "iter: 300 loss: 0.8584285658321076\n",
      "iter: 400 loss: 0.7963068958212689\n",
      "Acc: 0.7083333333333334\n",
      "###########################################\n",
      "epoch: 21\n",
      "iter: 0 loss: 0.7188077851509409\n",
      "iter: 100 loss: 0.8349026002398776\n",
      "iter: 200 loss: 0.7131833152361614\n",
      "iter: 300 loss: 0.8365401906833773\n",
      "iter: 400 loss: 0.7817314800833814\n",
      "Acc: 0.7083333333333334\n",
      "###########################################\n",
      "epoch: 22\n",
      "iter: 0 loss: 0.7017955428872703\n",
      "iter: 100 loss: 0.8129637119834358\n",
      "iter: 200 loss: 0.6945666832050944\n",
      "iter: 300 loss: 0.8163431451804014\n",
      "iter: 400 loss: 0.7682728602066863\n",
      "Acc: 0.7083333333333334\n",
      "###########################################\n",
      "epoch: 23\n",
      "iter: 0 loss: 0.6861119573697809\n",
      "iter: 100 loss: 0.7927319749409608\n",
      "iter: 200 loss: 0.6773766613524053\n",
      "iter: 300 loss: 0.7976473541193907\n",
      "iter: 400 loss: 0.7558037682193693\n",
      "Acc: 0.71875\n",
      "###########################################\n",
      "epoch: 24\n",
      "iter: 0 loss: 0.6716101954854178\n",
      "iter: 100 loss: 0.7740147730327253\n",
      "iter: 200 loss: 0.6614579968353286\n",
      "iter: 300 loss: 0.7802898866928023\n",
      "iter: 400 loss: 0.7442147214575398\n",
      "Acc: 0.7291666666666666\n",
      "###########################################\n",
      "epoch: 25\n",
      "iter: 0 loss: 0.6581637995972692\n",
      "iter: 100 loss: 0.7566467825769123\n",
      "iter: 200 loss: 0.6466768206606\n",
      "iter: 300 loss: 0.7641303027503596\n",
      "iter: 400 loss: 0.7334112087485114\n",
      "Acc: 0.7395833333333334\n",
      "###########################################\n",
      "epoch: 26\n",
      "iter: 0 loss: 0.6456632391170816\n",
      "iter: 100 loss: 0.7404854202632292\n",
      "iter: 200 loss: 0.6329171148871401\n",
      "iter: 300 loss: 0.7490469324575215\n",
      "iter: 400 loss: 0.7233113685785092\n",
      "Acc: 0.7395833333333334\n",
      "###########################################\n",
      "epoch: 27\n",
      "iter: 0 loss: 0.6340131618129459\n",
      "iter: 100 loss: 0.7254071594201584\n",
      "iter: 200 loss: 0.6200778594620371\n",
      "iter: 300 loss: 0.7349338736592167\n",
      "iter: 400 loss: 0.7138440607532333\n",
      "Acc: 0.75\n",
      "###########################################\n",
      "epoch: 28\n",
      "iter: 0 loss: 0.6231301801490263\n",
      "iter: 100 loss: 0.7113045274622747\n",
      "iter: 200 loss: 0.6080707092740638\n",
      "iter: 300 loss: 0.7216985477996644\n",
      "iter: 400 loss: 0.7049472578160548\n",
      "Acc: 0.7604166666666666\n",
      "###########################################\n",
      "epoch: 29\n",
      "iter: 0 loss: 0.6129410710103071\n",
      "iter: 100 loss: 0.6980836424622424\n",
      "iter: 200 loss: 0.5968180889150737\n",
      "iter: 300 loss: 0.7092596953091606\n",
      "iter: 400 loss: 0.6965666990512747\n",
      "Acc: 0.7708333333333334\n",
      "###########################################\n",
      "epoch: 30\n",
      "iter: 0 loss: 0.6033812981472578\n",
      "iter: 100 loss: 0.685662179949437\n",
      "iter: 200 loss: 0.5862516194978042\n",
      "iter: 300 loss: 0.697545720162821\n",
      "iter: 400 loss: 0.6886547615255351\n",
      "Acc: 0.78125\n",
      "###########################################\n",
      "epoch: 31\n",
      "iter: 0 loss: 0.5943937891620726\n",
      "iter: 100 loss: 0.6739676856554879\n",
      "iter: 200 loss: 0.5763108116494519\n",
      "iter: 300 loss: 0.6864933143139129\n",
      "iter: 400 loss: 0.6811695112074486\n",
      "Acc: 0.8020833333333334\n",
      "###########################################\n",
      "epoch: 32\n",
      "iter: 0 loss: 0.5859279153015018\n",
      "iter: 100 loss: 0.6629361683751499\n",
      "iter: 200 loss: 0.5669419735112479\n",
      "iter: 300 loss: 0.6760463082312452\n",
      "iter: 400 loss: 0.6740739038359055\n",
      "Acc: 0.8020833333333334\n",
      "###########################################\n",
      "epoch: 33\n",
      "iter: 0 loss: 0.577938634428944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss: 0.6525109210690715\n",
      "iter: 200 loss: 0.558097293633748\n",
      "iter: 300 loss: 0.6661547054022547\n",
      "iter: 400 loss: 0.6673351104973216\n",
      "Acc: 0.8020833333333334\n",
      "###########################################\n",
      "epoch: 34\n",
      "iter: 0 loss: 0.5703857665258625\n",
      "iter: 100 loss: 0.6426415290002734\n",
      "iter: 200 loss: 0.5497340670555891\n",
      "iter: 300 loss: 0.6567738674828829\n",
      "iter: 400 loss: 0.660923947180292\n",
      "Acc: 0.8125\n",
      "###########################################\n",
      "epoch: 35\n",
      "iter: 0 loss: 0.5632333777783767\n",
      "iter: 100 loss: 0.6332830319248628\n",
      "iter: 200 loss: 0.5418140392926648\n",
      "iter: 300 loss: 0.6478638235357834\n",
      "iter: 400 loss: 0.6548143911243027\n",
      "Acc: 0.8229166666666666\n",
      "###########################################\n",
      "epoch: 36\n",
      "iter: 0 loss: 0.5564492543541517\n",
      "iter: 100 loss: 0.6243952137657164\n",
      "iter: 200 loss: 0.5343028479481169\n",
      "iter: 300 loss: 0.6393886820319234\n",
      "iter: 400 loss: 0.6489831697157484\n",
      "Acc: 0.8229166666666666\n",
      "###########################################\n",
      "epoch: 37\n",
      "iter: 0 loss: 0.5500044508101938\n",
      "iter: 100 loss: 0.6159419982320083\n",
      "iter: 200 loss: 0.5271695455445976\n",
      "iter: 300 loss: 0.6313161283787738\n",
      "iter: 400 loss: 0.6434094101182289\n",
      "Acc: 0.8229166666666666\n",
      "###########################################\n",
      "epoch: 38\n",
      "iter: 0 loss: 0.543872901013158\n",
      "iter: 100 loss: 0.6078909328337248\n",
      "iter: 200 loss: 0.5203861902436737\n",
      "iter: 300 loss: 0.6236169939577403\n",
      "iter: 400 loss: 0.6380743398398693\n",
      "Acc: 0.8229166666666666\n",
      "###########################################\n",
      "epoch: 39\n",
      "iter: 0 loss: 0.5380310817313294\n",
      "iter: 100 loss: 0.600212746919552\n",
      "iter: 200 loss: 0.5139274935477597\n",
      "iter: 300 loss: 0.6162648852077638\n",
      "iter: 400 loss: 0.6329610301082078\n",
      "Acc: 0.8229166666666666\n",
      "###########################################\n",
      "epoch: 40\n",
      "iter: 0 loss: 0.5324577208391756\n",
      "iter: 100 loss: 0.5928809719183161\n",
      "iter: 200 loss: 0.5077705160218745\n",
      "iter: 300 loss: 0.6092358633324934\n",
      "iter: 400 loss: 0.6280541753014987\n",
      "Acc: 0.8229166666666666\n",
      "###########################################\n",
      "epoch: 41\n",
      "iter: 0 loss: 0.527133543483466\n",
      "iter: 100 loss: 0.5858716140241055\n",
      "iter: 200 loss: 0.5018944036342687\n",
      "iter: 300 loss: 0.6025081668485378\n",
      "iter: 400 loss: 0.6233399028231702\n",
      "Acc: 0.8229166666666666\n",
      "###########################################\n",
      "epoch: 42\n",
      "iter: 0 loss: 0.5220410506838491\n",
      "iter: 100 loss: 0.5791628712364465\n",
      "iter: 200 loss: 0.49628015857816854\n",
      "iter: 300 loss: 0.5960619705183925\n",
      "iter: 400 loss: 0.6188056087453621\n",
      "Acc: 0.8229166666666666\n",
      "###########################################\n",
      "epoch: 43\n",
      "iter: 0 loss: 0.517164325746242\n",
      "iter: 100 loss: 0.572734888029004\n",
      "iter: 200 loss: 0.4909104394641739\n",
      "iter: 300 loss: 0.5898791752893559\n",
      "iter: 400 loss: 0.6144398153225553\n",
      "Acc: 0.8229166666666666\n",
      "###########################################\n",
      "epoch: 44\n",
      "iter: 0 loss: 0.512488864603422\n",
      "iter: 100 loss: 0.5665695420349014\n",
      "iter: 200 loss: 0.4857693866122865\n",
      "iter: 300 loss: 0.5839432247402269\n",
      "iter: 400 loss: 0.6102320471163575\n",
      "Acc: 0.8229166666666666\n",
      "###########################################\n",
      "epoch: 45\n",
      "iter: 0 loss: 0.5080014268002185\n",
      "iter: 100 loss: 0.5606502580523254\n",
      "iter: 200 loss: 0.48084246886140486\n",
      "iter: 300 loss: 0.5782389442599847\n",
      "iter: 400 loss: 0.6061727230014532\n",
      "Acc: 0.8229166666666666\n",
      "###########################################\n",
      "epoch: 46\n",
      "iter: 0 loss: 0.5036899043384259\n",
      "iter: 100 loss: 0.5549618454286879\n",
      "iter: 200 loss: 0.47611634888188764\n",
      "iter: 300 loss: 0.5727523997780847\n",
      "iter: 400 loss: 0.6022530617604782\n",
      "Acc: 0.8229166666666666\n",
      "###########################################\n",
      "epoch: 47\n",
      "iter: 0 loss: 0.4995432060099846\n",
      "iter: 100 loss: 0.5494903555055438\n",
      "iter: 200 loss: 0.47157876444628527\n",
      "iter: 300 loss: 0.567470773358681\n",
      "iter: 400 loss: 0.5984649993384389\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 48\n",
      "iter: 0 loss: 0.4955511551923221\n",
      "iter: 100 loss: 0.5442229563239309\n",
      "iter: 200 loss: 0.46721842350309173\n",
      "iter: 300 loss: 0.5623822533803344\n",
      "iter: 400 loss: 0.5948011161284801\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 49\n",
      "iter: 0 loss: 0.49170439936971494\n",
      "iter: 100 loss: 0.5391478222202041\n",
      "iter: 200 loss: 0.46302491122288\n",
      "iter: 300 loss: 0.5574759373638557\n",
      "iter: 400 loss: 0.5912545729115394\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 50\n",
      "iter: 0 loss: 0.48799432988903063\n",
      "iter: 100 loss: 0.5342540363013086\n",
      "iter: 200 loss: 0.4589886074572941\n",
      "iter: 300 loss: 0.5527417457962894\n",
      "iter: 400 loss: 0.5878190542813827\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 51\n",
      "iter: 0 loss: 0.4844130106651861\n",
      "iter: 100 loss: 0.5295315040886625\n",
      "iter: 200 loss: 0.45510061327849366\n",
      "iter: 300 loss: 0.5481703455383768\n",
      "iter: 400 loss: 0.5844887185611938\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 52\n",
      "iter: 0 loss: 0.48095311472744023\n",
      "iter: 100 loss: 0.5249708768714344\n",
      "iter: 200 loss: 0.4513526854575191\n",
      "iter: 300 loss: 0.5437530816043871\n",
      "iter: 400 loss: 0.5812581533642696\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 53\n",
      "iter: 0 loss: 0.4776078676473504\n",
      "iter: 100 loss: 0.5205634835214822\n",
      "iter: 200 loss: 0.44773717790087275\n",
      "iter: 300 loss: 0.5394819162732004\n",
      "iter: 400 loss: 0.5781223360742291\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 54\n",
      "iter: 0 loss: 0.47437099701719543\n",
      "iter: 100 loss: 0.5163012697004749\n",
      "iter: 200 loss: 0.44424698920052563\n",
      "iter: 300 loss: 0.5353493746335614\n",
      "iter: 400 loss: 0.57507659862364\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 55\n",
      "iter: 0 loss: 0.47123668725710344\n",
      "iter: 100 loss: 0.5121767435402237\n",
      "iter: 200 loss: 0.44087551556772003\n",
      "iter: 300 loss: 0.5313484957885162\n",
      "iter: 400 loss: 0.5721165960372849\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 56\n",
      "iter: 0 loss: 0.4681995391231391\n",
      "iter: 100 loss: 0.5081829270046768\n",
      "iter: 200 loss: 0.4376166085187903\n",
      "iter: 300 loss: 0.5274727890480958\n",
      "iter: 400 loss: 0.5692382782802341\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 57\n",
      "iter: 0 loss: 0.4652545333693815\n",
      "iter: 100 loss: 0.5043133122502161\n",
      "iter: 200 loss: 0.4344645367645472\n",
      "iter: 300 loss: 0.523716194527926\n",
      "iter: 400 loss: 0.5664378650135637\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 58\n",
      "iter: 0 loss: 0.4623969980866302\n",
      "iter: 100 loss: 0.5005618223928608\n",
      "iter: 200 loss: 0.431413951825979\n",
      "iter: 300 loss: 0.5200730476473366\n",
      "iter: 400 loss: 0.5637118229138527\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 59\n",
      "iter: 0 loss: 0.45962257930042716\n",
      "iter: 100 loss: 0.49692277616944686\n",
      "iter: 200 loss: 0.42845985695998434\n",
      "iter: 300 loss: 0.5165380470855438\n",
      "iter: 400 loss: 0.561056845258161\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 60\n",
      "iter: 0 loss: 0.45692721446301576\n",
      "iter: 100 loss: 0.4933908560468766\n",
      "iter: 200 loss: 0.42559757903113804\n",
      "iter: 300 loss: 0.5131062258103108\n",
      "iter: 400 loss: 0.5584698335149161\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 61\n",
      "iter: 0 loss: 0.454307108518825\n",
      "iter: 100 loss: 0.48996107939090866\n",
      "iter: 200 loss: 0.42282274301057965\n",
      "iter: 300 loss: 0.5097729248416077\n",
      "iter: 400 loss: 0.5559478807145483\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 62\n",
      "iter: 0 loss: 0.4517587122620166\n",
      "iter: 100 loss: 0.48662877235530805\n",
      "iter: 200 loss: 0.420131248821887\n",
      "iter: 300 loss: 0.5065337694542758\n",
      "iter: 400 loss: 0.5534882564022366\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 63\n",
      "iter: 0 loss: 0.4492787027385449\n",
      "iter: 100 loss: 0.4833895461944816\n",
      "iter: 200 loss: 0.4175192502874788\n",
      "iter: 300 loss: 0.5033846475596144\n",
      "iter: 400 loss: 0.5510883929997655\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 64\n",
      "iter: 0 loss: 0.4468639654746005\n",
      "iter: 100 loss: 0.4802392757393357\n",
      "iter: 200 loss: 0.41498313595811\n",
      "iter: 300 loss: 0.5003216900368537\n",
      "iter: 400 loss: 0.5487458734247318\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 65\n",
      "iter: 0 loss: 0.4445115783389936\n",
      "iter: 100 loss: 0.47717407980765014\n",
      "iter: 200 loss: 0.4125195116333965\n",
      "iter: 300 loss: 0.49734125281253666\n",
      "iter: 400 loss: 0.5464584198336708\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 66\n",
      "iter: 0 loss: 0.4422187968694244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss: 0.47419030334757895\n",
      "iter: 200 loss: 0.41012518440328133\n",
      "iter: 300 loss: 0.4944399005093032\n",
      "iter: 400 loss: 0.5442238833716408\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 67\n",
      "iter: 0 loss: 0.43998304091214135\n",
      "iter: 100 loss: 0.4712845011366501\n",
      "iter: 200 loss: 0.40779714805958583\n",
      "iter: 300 loss: 0.4916143915060308\n",
      "iter: 400 loss: 0.5420402348245392\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 68\n",
      "iter: 0 loss: 0.43780188244161755\n",
      "iter: 100 loss: 0.4684534228792316\n",
      "iter: 200 loss: 0.405532569743606\n",
      "iter: 300 loss: 0.488861664269201\n",
      "iter: 400 loss: 0.5399055560825156\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 69\n",
      "iter: 0 loss: 0.43567303444189626\n",
      "iter: 100 loss: 0.46569399956337754\n",
      "iter: 200 loss: 0.40332877771039893\n",
      "iter: 300 loss: 0.4861788248309922\n",
      "iter: 400 loss: 0.5378180323332816\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 70\n",
      "iter: 0 loss: 0.43359434074438474\n",
      "iter: 100 loss: 0.46300333095371254\n",
      "iter: 200 loss: 0.40118325010338685\n",
      "iter: 300 loss: 0.4835631353033292\n",
      "iter: 400 loss: 0.5357759449132949\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 71\n",
      "iter: 0 loss: 0.43156376672848185\n",
      "iter: 100 loss: 0.46037867411065003\n",
      "iter: 200 loss: 0.3990936046442216\n",
      "iter: 300 loss: 0.48101200332916144\n",
      "iter: 400 loss: 0.533777664752797\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 72\n",
      "iter: 0 loss: 0.42957939080159574\n",
      "iter: 100 loss: 0.45781743283831144\n",
      "iter: 200 loss: 0.3970575891529194\n",
      "iter: 300 loss: 0.4785229723828274\n",
      "iter: 400 loss: 0.5318216463577096\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 73\n",
      "iter: 0 loss: 0.4276393965840676\n",
      "iter: 100 loss: 0.4553171479740498\n",
      "iter: 200 loss: 0.39507307282209214\n",
      "iter: 300 loss: 0.47609371284073143\n",
      "iter: 400 loss: 0.5299064222775723\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 74\n",
      "iter: 0 loss: 0.4257420657324825\n",
      "iter: 100 loss: 0.45287548844174697\n",
      "iter: 200 loss: 0.3931380381769543\n",
      "iter: 300 loss: 0.4737220137517687\n",
      "iter: 400 loss: 0.5280305980141314\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 75\n",
      "iter: 0 loss: 0.4238857713417746\n",
      "iter: 100 loss: 0.4504902429992458\n",
      "iter: 200 loss: 0.3912505736596992\n",
      "iter: 300 loss: 0.4714057752442373\n",
      "iter: 400 loss: 0.5261928473299624\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 76\n",
      "iter: 0 loss: 0.42206897187280906\n",
      "iter: 100 loss: 0.4481593126175024\n",
      "iter: 200 loss: 0.3894088667830129\n",
      "iter: 300 loss: 0.46914300151242566\n",
      "iter: 400 loss: 0.5243919079207789\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 77\n",
      "iter: 0 loss: 0.4202902055575423\n",
      "iter: 100 loss: 0.4458807034353836\n",
      "iter: 200 loss: 0.3876111978029406\n",
      "iter: 300 loss: 0.4669317943317969\n",
      "iter: 400 loss: 0.5226265774187558\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 78\n",
      "iter: 0 loss: 0.41854808523877884\n",
      "iter: 100 loss: 0.44365252023977036\n",
      "iter: 200 loss: 0.3858559338662242\n",
      "iter: 300 loss: 0.46477034705673803\n",
      "iter: 400 loss: 0.5208957096975597\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 79\n",
      "iter: 0 loss: 0.4168412936058468\n",
      "iter: 100 loss: 0.4414729604255348\n",
      "iter: 200 loss: 0.38414152359153636\n",
      "iter: 300 loss: 0.46265693905942623\n",
      "iter: 400 loss: 0.5191982114526895\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 80\n",
      "iter: 0 loss: 0.4151685787913514\n",
      "iter: 100 loss: 0.43934030839451904\n",
      "iter: 200 loss: 0.38246649204795624\n",
      "iter: 300 loss: 0.46058993057231945\n",
      "iter: 400 loss: 0.5175330390333541\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 81\n",
      "iter: 0 loss: 0.4135287502976435\n",
      "iter: 100 loss: 0.4372529303565383\n",
      "iter: 200 loss: 0.3808294360974495\n",
      "iter: 300 loss: 0.4585677579004356\n",
      "iter: 400 loss: 0.5158991955044322\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 82\n",
      "iter: 0 loss: 0.41192067522462233\n",
      "iter: 100 loss: 0.4352092694990092\n",
      "iter: 200 loss: 0.37922902007128917\n",
      "iter: 300 loss: 0.45658892897275355\n",
      "iter: 400 loss: 0.5142957279191172\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 83\n",
      "iter: 0 loss: 0.41034327477329546\n",
      "iter: 100 loss: 0.43320784149493546\n",
      "iter: 200 loss: 0.3776639717530469\n",
      "iter: 300 loss: 0.4546520192049614\n",
      "iter: 400 loss: 0.5127217247847432\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 84\n",
      "iter: 0 loss: 0.4087955210018999\n",
      "iter: 100 loss: 0.43124723032183127\n",
      "iter: 200 loss: 0.3761330786433956\n",
      "iter: 300 loss: 0.45275566764834707\n",
      "iter: 400 loss: 0.5111763137058668\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 85\n",
      "iter: 0 loss: 0.40727643381362044\n",
      "iter: 100 loss: 0.4293260843666733\n",
      "iter: 200 loss: 0.37463518448410127\n",
      "iter: 300 loss: 0.4508985734019277\n",
      "iter: 400 loss: 0.5096586591902458\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 86\n",
      "iter: 0 loss: 0.40578507815686354\n",
      "iter: 100 loss: 0.4274431127942382\n",
      "iter: 200 loss: 0.3731691860206906\n",
      "iter: 300 loss: 0.44907949226699206\n",
      "iter: 400 loss: 0.5081679606046033\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 87\n",
      "iter: 0 loss: 0.4043205614208118\n",
      "iter: 100 loss: 0.42559708215823855\n",
      "iter: 200 loss: 0.37173402998504934\n",
      "iter: 300 loss: 0.44729723362510615\n",
      "iter: 400 loss: 0.5067034502682974\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 88\n",
      "iter: 0 loss: 0.40288203101058334\n",
      "iter: 100 loss: 0.4237868132364841\n",
      "iter: 200 loss: 0.370328710280875\n",
      "iter: 300 loss: 0.44555065752227685\n",
      "iter: 400 loss: 0.5052643916740885\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 89\n",
      "iter: 0 loss: 0.4014686720876944\n",
      "iter: 100 loss: 0.42201117807294053\n",
      "iter: 200 loss: 0.3689522653564009\n",
      "iter: 300 loss: 0.4438386719435213\n",
      "iter: 400 loss: 0.5038500778261316\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 90\n",
      "iter: 0 loss: 0.40007970546284705\n",
      "iter: 100 loss: 0.4202690972110633\n",
      "iter: 200 loss: 0.36760377575010605\n",
      "iter: 300 loss: 0.44216023026343637\n",
      "iter: 400 loss: 0.5024598296862126\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 91\n",
      "iter: 0 loss: 0.3987143856291814\n",
      "iter: 100 loss: 0.4185595371041076\n",
      "iter: 200 loss: 0.366282361796407\n",
      "iter: 300 loss: 0.4405143288595782\n",
      "iter: 400 loss: 0.5010929947200066\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 92\n",
      "iter: 0 loss: 0.39737199892518743\n",
      "iter: 100 loss: 0.4168815076893449\n",
      "iter: 200 loss: 0.3649871814793659\n",
      "iter: 300 loss: 0.43890000487662656\n",
      "iter: 400 loss: 0.4997489455358745\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 93\n",
      "iter: 0 loss: 0.3960518618174019\n",
      "iter: 100 loss: 0.41523406011418423\n",
      "iter: 200 loss: 0.36371742842348453\n",
      "iter: 300 loss: 0.4373163341302639\n",
      "iter: 400 loss: 0.49842707860930985\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 94\n",
      "iter: 0 loss: 0.3947533192938661\n",
      "iter: 100 loss: 0.4136162846032372\n",
      "iter: 200 loss: 0.36247233001155676\n",
      "iter: 300 loss: 0.4357624291406559\n",
      "iter: 400 loss: 0.49712681308676404\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 95\n",
      "iter: 0 loss: 0.3934757433600808\n",
      "iter: 100 loss: 0.4120273084562255\n",
      "iter: 200 loss: 0.36125114562034555\n",
      "iter: 300 loss: 0.4342374372862305\n",
      "iter: 400 loss: 0.4958475896630758\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 96\n",
      "iter: 0 loss: 0.3922185316299102\n",
      "iter: 100 loss: 0.41046629416746344\n",
      "iter: 200 loss: 0.36005316496564893\n",
      "iter: 300 loss: 0.4327405390692223\n",
      "iter: 400 loss: 0.49458886952722797\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 97\n",
      "iter: 0 loss: 0.39098110600447494\n",
      "iter: 100 loss: 0.4089324376584104\n",
      "iter: 200 loss: 0.3588777065489507\n",
      "iter: 300 loss: 0.4312709464851163\n",
      "iter: 400 loss: 0.49335013337155426\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 98\n",
      "iter: 0 loss: 0.3897629114327126\n",
      "iter: 100 loss: 0.4074249666154285\n",
      "iter: 200 loss: 0.3577241161985116\n",
      "iter: 300 loss: 0.4298279014887753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 400 loss: 0.492130880459946\n",
      "Acc: 0.8333333333333334\n",
      "###########################################\n",
      "epoch: 99\n",
      "iter: 0 loss: 0.38856341474771766\n",
      "iter: 100 loss: 0.40594313892552775\n",
      "iter: 200 loss: 0.35659176569829243\n",
      "iter: 300 loss: 0.42841067455059234\n",
      "iter: 400 loss: 0.49093062775094154\n",
      "Acc: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "#for i in range(0, len(train_data), batch):\n",
    "for e in range(100):\n",
    "    print('###########################################') \n",
    "    print('epoch:',e)\n",
    "    for i in range(len(train_data)//batch):\n",
    "    #for i in range(1):\n",
    "        #g1, b1, g2, b2=0,0,0,0\n",
    "        g1, b1=0,0\n",
    "        loss=0 \n",
    "        for j in range(i*batch, (i+1)*batch):\n",
    "            pred=layer.forward(train_data[j])\n",
    "            loss+=crossEntropy(pred, train_label[j])\n",
    "            g1, b1=layer.backward(pred, train_label[j], g1, b1)\n",
    "        layer.update(g1,b1)\n",
    "        if i%100==0:\n",
    "            print('iter:',i, 'loss:', loss/batch)\n",
    "    result=[]\n",
    "    for i in range(len(val_data)):\n",
    "        result.append(np.argmax(layer.forward(val_data[i])))\n",
    "    count=0\n",
    "    for l, r in zip(val_label, result):\n",
    "        if np.argmax(l)==r:\n",
    "            count+=1\n",
    "    print('Acc:', count/len(val_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8673\n"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "for i in range(len(test_data)):\n",
    "    result.append(np.argmax(layer.forward(test_data[i])))\n",
    "count=0\n",
    "for l, r in zip(test_label, result):\n",
    "    if np.argmax(l)==r:\n",
    "        count+=1\n",
    "print('Test Accuracy:', count/len(test_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
